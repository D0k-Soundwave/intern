# Ollama API Integration Guide with OAuth 2.1 & HTML Functions
*Last Updated: August 2025*

## Table of Contents
1. [Overview](#overview)
2. [Core API Endpoints](#core-api-endpoints)
3. [Authentication & Security](#authentication--security)
4. [OAuth 2.1 Integration](#oauth-21-integration)
5. [HTML & JavaScript Integration](#html--javascript-integration)
6. [Structured Outputs & Tool Calling](#structured-outputs--tool-calling)
7. [Best Practices](#best-practices)
8. [Production Deployment](#production-deployment)
9. [Code Examples](#code-examples)

---

## Overview

Ollama is an open-source platform for running Large Language Models (LLMs) locally. It provides a REST API on port 11434 that enables seamless integration with web applications, offering both streaming and non-streaming responses.

### Key Features (2025)
- Local model execution with no cloud dependencies
- REST API with multiple endpoints
- Streaming and non-streaming responses
- Tool/function calling support
- Structured JSON output with schema validation
- WebSocket support for real-time applications
- Multiple language SDKs (JavaScript, Python, Go)

### System Requirements
- **Ollama Version**: 0.1.25+ (required for authentication features)
- **Port**: Default 11434 (configurable)
- **Protocol**: HTTP/HTTPS (with reverse proxy)
- **Memory**: Varies by model (4GB-32GB+ RAM)

---

## Core API Endpoints

### Base URL
```
http://localhost:11434
```

### 1. Generate Completion
**Endpoint**: `POST /api/generate`

```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2",
    "prompt": "Write a haiku about coding",
    "stream": false,
    "options": {
      "temperature": 0.7,
      "top_p": 0.9,
      "max_tokens": 100
    }
  }'
```

**Response Structure**:
```json
{
  "model": "llama3.2",
  "created_at": "2025-08-31T10:00:00Z",
  "response": "Generated text here",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 1000000000,
  "load_duration": 500000000,
  "prompt_eval_count": 10,
  "prompt_eval_duration": 300000000,
  "eval_count": 20,
  "eval_duration": 200000000
}
```

### 2. Chat Completion
**Endpoint**: `POST /api/chat`

```bash
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant"},
      {"role": "user", "content": "Explain quantum computing"}
    ],
    "stream": true,
    "tools": []
  }'
```

### 3. List Available Models
**Endpoint**: `GET /api/tags`

```bash
curl http://localhost:11434/api/tags
```

**Response**:
```json
{
  "models": [
    {
      "name": "llama3.2:latest",
      "modified_at": "2025-08-30T10:00:00Z",
      "size": 4000000000,
      "digest": "sha256:abc123..."
    }
  ]
}
```

### 4. Model Information
**Endpoint**: `POST /api/show`

```bash
curl -X POST http://localhost:11434/api/show \
  -d '{"name": "llama3.2"}'
```

### 5. Pull Model
**Endpoint**: `POST /api/pull`

```bash
curl -X POST http://localhost:11434/api/pull \
  -d '{"name": "llama3.2", "stream": true}'
```

### 6. Push Model
**Endpoint**: `POST /api/push`

### 7. Create Model
**Endpoint**: `POST /api/create`

### 8. Copy Model
**Endpoint**: `POST /api/copy`

### 9. Delete Model
**Endpoint**: `DELETE /api/delete`

### 10. Embeddings
**Endpoint**: `POST /api/embeddings`

```bash
curl -X POST http://localhost:11434/api/embeddings \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Generate embeddings for this text"
  }'
```

---

## Authentication & Security

### ⚠️ Security Warning
**By default, Ollama runs without authentication and listens on 127.0.0.1**. A critical vulnerability (CNVD-2025-04094) was disclosed where unauthenticated attackers can directly call the API when exposed to public networks.

### Security Implementation Layers

#### 1. Bearer Token Authentication
```bash
# Generate secure token
openssl rand -hex 32 > auth_token.txt

# Start Ollama with authentication
ollama serve --auth-token $(cat auth_token.txt)
```

**Client Usage**:
```javascript
const response = await fetch('http://localhost:11434/api/generate', {
  method: 'POST',
  headers: {
    'Authorization': 'Bearer YOUR_TOKEN_HERE',
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'llama3.2',
    prompt: 'Hello world'
  })
});
```

#### 2. IP Whitelisting
```yaml
# config.yaml
server:
  allowed_ips:
    - 127.0.0.1
    - 192.168.1.0/24
    - 10.0.0.0/8
```

#### 3. Rate Limiting
```nginx
# nginx.conf
http {
  limit_req_zone $binary_remote_addr zone=ollama:10m rate=10r/s;
  
  server {
    location /api/ {
      limit_req zone=ollama burst=20 nodelay;
      proxy_pass http://localhost:11434;
    }
  }
}
```

---

## OAuth 2.1 Integration

Since Ollama doesn't have native OAuth 2.1 support, implement it through a reverse proxy or API gateway:

### 1. FastAPI OAuth Wrapper

```python
from fastapi import FastAPI, Depends, HTTPException, Security
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from jose import JWTError, jwt
from datetime import datetime, timedelta
import httpx
import os

app = FastAPI()

# OAuth2 Configuration
ALGORITHM = "HS256"
SECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key")
ACCESS_TOKEN_EXPIRE_MINUTES = 30

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

# OAuth 2.1 compliant token endpoint
@app.post("/token")
async def login(form_data: OAuth2PasswordRequestForm = Depends()):
    # Validate against your auth provider (e.g., Auth0, Okta)
    user = authenticate_user(form_data.username, form_data.password)
    if not user:
        raise HTTPException(status_code=401, detail="Invalid credentials")
    
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user.username}, 
        expires_delta=access_token_expires
    )
    
    return {
        "access_token": access_token,
        "token_type": "bearer",
        "expires_in": ACCESS_TOKEN_EXPIRE_MINUTES * 60
    }

# Protected Ollama endpoint
@app.post("/api/generate")
async def generate(
    request: dict,
    token: str = Depends(oauth2_scheme)
):
    # Verify token
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username = payload.get("sub")
        if username is None:
            raise HTTPException(status_code=401)
    except JWTError:
        raise HTTPException(status_code=401)
    
    # Forward to Ollama
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:11434/api/generate",
            json=request,
            timeout=30.0
        )
        return response.json()

def create_access_token(data: dict, expires_delta: timedelta = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
```

### 2. Express.js OAuth Middleware

```javascript
const express = require('express');
const { auth } = require('express-oauth-server');
const httpProxy = require('http-proxy-middleware');

const app = express();

// OAuth 2.1 server setup
const oauth = new OAuth2Server({
  model: require('./oauth-model'),
  grants: ['authorization_code', 'refresh_token'],
  accessTokenLifetime: 3600,
  allowBearerTokensInQueryString: false,
  requireClientAuthentication: {
    authorization_code: true,
    refresh_token: true
  }
});

// OAuth middleware
app.use('/api/*', auth({
  scope: 'ollama:access'
}));

// Proxy to Ollama
app.use('/api', httpProxy.createProxyMiddleware({
  target: 'http://localhost:11434',
  changeOrigin: true,
  onProxyReq: (proxyReq, req, res) => {
    // Add any additional headers
    proxyReq.setHeader('X-User-Id', req.user.id);
  }
}));

app.listen(3000);
```

### 3. Azure API Management Integration

```xml
<!-- Azure APIM Policy -->
<policies>
  <inbound>
    <validate-jwt header-name="Authorization" 
                  failed-validation-httpcode="401"
                  failed-validation-error-message="Unauthorized">
      <openid-config url="https://login.microsoftonline.com/{tenant}/.well-known/openid-configuration" />
      <audiences>
        <audience>api://ollama-api</audience>
      </audiences>
    </validate-jwt>
    <rate-limit calls="100" renewal-period="60" />
    <set-backend-service base-url="http://your-ollama-server:11434" />
  </inbound>
  <backend>
    <forward-request />
  </backend>
  <outbound>
    <set-header name="X-Frame-Options" exists-action="override">
      <value>DENY</value>
    </set-header>
  </outbound>
</policies>
```

---

## HTML & JavaScript Integration

### 1. Vanilla JavaScript with Fetch API

```html
<!DOCTYPE html>
<html>
<head>
  <title>Ollama Chat Interface</title>
  <style>
    #chat-container {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
    }
    #messages {
      height: 400px;
      overflow-y: auto;
      border: 1px solid #ccc;
      padding: 10px;
      margin-bottom: 10px;
    }
    .message {
      margin: 10px 0;
      padding: 10px;
      border-radius: 5px;
    }
    .user { background: #e3f2fd; }
    .assistant { background: #f5f5f5; }
    #input-area {
      display: flex;
      gap: 10px;
    }
    #user-input {
      flex: 1;
      padding: 10px;
    }
  </style>
</head>
<body>
  <div id="chat-container">
    <h1>Ollama Chat</h1>
    <div id="messages"></div>
    <div id="input-area">
      <input type="text" id="user-input" placeholder="Type your message...">
      <button onclick="sendMessage()">Send</button>
    </div>
  </div>

  <script>
    const API_URL = 'http://localhost:11434';
    let conversationHistory = [];

    async function sendMessage() {
      const input = document.getElementById('user-input');
      const userMessage = input.value.trim();
      if (!userMessage) return;

      // Add user message to UI
      appendMessage('user', userMessage);
      input.value = '';

      // Add to conversation history
      conversationHistory.push({ role: 'user', content: userMessage });

      try {
        // Stream response
        const response = await fetch(`${API_URL}/api/chat`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            // Add OAuth token if implemented
            // 'Authorization': 'Bearer ' + getAccessToken()
          },
          body: JSON.stringify({
            model: 'llama3.2',
            messages: conversationHistory,
            stream: true
          })
        });

        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let assistantMessage = '';

        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          const chunk = decoder.decode(value);
          const lines = chunk.split('\n');

          for (const line of lines) {
            if (line.trim()) {
              try {
                const data = JSON.parse(line);
                if (data.message?.content) {
                  assistantMessage += data.message.content;
                  updateLastMessage('assistant', assistantMessage);
                }
              } catch (e) {
                console.error('Parse error:', e);
              }
            }
          }
        }

        // Add assistant response to history
        conversationHistory.push({ 
          role: 'assistant', 
          content: assistantMessage 
        });

      } catch (error) {
        console.error('Error:', error);
        appendMessage('assistant', 'Error: Failed to get response');
      }
    }

    function appendMessage(role, content) {
      const messagesDiv = document.getElementById('messages');
      const messageDiv = document.createElement('div');
      messageDiv.className = `message ${role}`;
      messageDiv.textContent = content;
      messagesDiv.appendChild(messageDiv);
      messagesDiv.scrollTop = messagesDiv.scrollHeight;
    }

    function updateLastMessage(role, content) {
      const messages = document.querySelectorAll('.message');
      const lastMessage = messages[messages.length - 1];
      
      if (lastMessage && lastMessage.classList.contains(role)) {
        lastMessage.textContent = content;
      } else {
        appendMessage(role, content);
      }
    }

    // Enter key support
    document.getElementById('user-input').addEventListener('keypress', (e) => {
      if (e.key === 'Enter') sendMessage();
    });
  </script>
</body>
</html>
```

### 2. React Integration

```jsx
import React, { useState, useEffect } from 'react';
import { Ollama } from 'ollama/browser';

const OllamaChat = () => {
  const [ollama, setOllama] = useState(null);
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);

  useEffect(() => {
    // Initialize Ollama client
    const client = new Ollama({
      host: process.env.REACT_APP_OLLAMA_HOST || 'http://localhost:11434'
    });
    setOllama(client);
  }, []);

  const sendMessage = async () => {
    if (!input.trim() || !ollama) return;

    const userMessage = { role: 'user', content: input };
    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);

    try {
      const response = await ollama.chat({
        model: 'llama3.2',
        messages: [...messages, userMessage],
        stream: true
      });

      let assistantMessage = '';
      setMessages(prev => [...prev, { role: 'assistant', content: '' }]);

      for await (const part of response) {
        assistantMessage += part.message.content;
        setMessages(prev => {
          const newMessages = [...prev];
          newMessages[newMessages.length - 1].content = assistantMessage;
          return newMessages;
        });
      }
    } catch (error) {
      console.error('Chat error:', error);
      setMessages(prev => [...prev, { 
        role: 'assistant', 
        content: 'Error: Failed to get response' 
      }]);
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="chat-container">
      <div className="messages">
        {messages.map((msg, idx) => (
          <div key={idx} className={`message ${msg.role}`}>
            {msg.content}
          </div>
        ))}
      </div>
      <div className="input-area">
        <input
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && sendMessage()}
          placeholder="Type your message..."
          disabled={isLoading}
        />
        <button onClick={sendMessage} disabled={isLoading}>
          {isLoading ? 'Sending...' : 'Send'}
        </button>
      </div>
    </div>
  );
};

export default OllamaChat;
```

### 3. WebSocket Implementation

```javascript
class OllamaWebSocket {
  constructor(url = 'ws://localhost:8080') {
    this.url = url;
    this.ws = null;
    this.messageHandlers = [];
  }

  connect() {
    return new Promise((resolve, reject) => {
      this.ws = new WebSocket(this.url);

      this.ws.onopen = () => {
        console.log('WebSocket connected');
        resolve();
      };

      this.ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        this.messageHandlers.forEach(handler => handler(data));
      };

      this.ws.onerror = (error) => {
        console.error('WebSocket error:', error);
        reject(error);
      };

      this.ws.onclose = () => {
        console.log('WebSocket disconnected');
        // Attempt reconnection
        setTimeout(() => this.connect(), 5000);
      };
    });
  }

  sendMessage(message) {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      this.ws.send(JSON.stringify({
        type: 'chat',
        model: 'llama3.2',
        message: message
      }));
    }
  }

  onMessage(handler) {
    this.messageHandlers.push(handler);
  }

  disconnect() {
    if (this.ws) {
      this.ws.close();
    }
  }
}

// Usage
const ollamaWS = new OllamaWebSocket();

ollamaWS.onMessage((data) => {
  console.log('Received:', data);
  // Update UI with response
});

await ollamaWS.connect();
ollamaWS.sendMessage('Hello, Ollama!');
```

---

## Structured Outputs & Tool Calling

### 1. JSON Schema Structured Output

```javascript
// Define JSON schema
const schema = {
  type: 'object',
  properties: {
    name: { type: 'string' },
    age: { type: 'number' },
    skills: {
      type: 'array',
      items: { type: 'string' }
    }
  },
  required: ['name', 'age']
};

// Request with schema
const response = await fetch('http://localhost:11434/api/generate', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'llama3.2',
    prompt: 'Generate a person profile',
    format: schema,
    stream: false
  })
});

const data = await response.json();
// Response will conform to the schema
console.log(JSON.parse(data.response));
```

### 2. Tool/Function Calling

```javascript
// Define tools
const tools = [
  {
    type: 'function',
    function: {
      name: 'get_weather',
      description: 'Get weather for a location',
      parameters: {
        type: 'object',
        properties: {
          location: {
            type: 'string',
            description: 'City name'
          },
          unit: {
            type: 'string',
            enum: ['celsius', 'fahrenheit']
          }
        },
        required: ['location']
      }
    }
  }
];

// Chat with tools
const response = await fetch('http://localhost:11434/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'llama3.2',
    messages: [
      { role: 'user', content: "What's the weather in Tokyo?" }
    ],
    tools: tools,
    stream: false
  })
});

const result = await response.json();

// Check if tool was called
if (result.message.tool_calls) {
  for (const toolCall of result.message.tool_calls) {
    console.log('Tool called:', toolCall.function.name);
    console.log('Arguments:', toolCall.function.arguments);
    
    // Execute the actual function
    const weatherData = await getWeather(
      JSON.parse(toolCall.function.arguments)
    );
    
    // Send result back to model
    const followUp = await fetch('http://localhost:11434/api/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'llama3.2',
        messages: [
          ...messages,
          {
            role: 'tool',
            content: JSON.stringify(weatherData)
          }
        ]
      })
    });
  }
}
```

### 3. Python Implementation with Pydantic

```python
from pydantic import BaseModel
from typing import List
import ollama

class PersonProfile(BaseModel):
    name: str
    age: int
    skills: List[str]
    bio: str

# Generate structured output
client = ollama.Client()
response = client.generate(
    model='llama3.2',
    prompt='Create a profile for a software developer',
    format=PersonProfile.model_json_schema()
)

# Parse response
profile = PersonProfile.model_validate_json(response['response'])
print(f"Name: {profile.name}")
print(f"Skills: {', '.join(profile.skills)}")
```

---

## Best Practices

### 1. Error Handling

```javascript
class OllamaClient {
  constructor(host = 'http://localhost:11434') {
    this.host = host;
    this.timeout = 30000;
    this.retryAttempts = 3;
  }

  async request(endpoint, data, attempt = 1) {
    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), this.timeout);

      const response = await fetch(`${this.host}${endpoint}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(data),
        signal: controller.signal
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      return await response.json();

    } catch (error) {
      if (attempt < this.retryAttempts) {
        console.log(`Retry attempt ${attempt + 1}...`);
        await this.delay(1000 * attempt);
        return this.request(endpoint, data, attempt + 1);
      }
      
      throw error;
    }
  }

  delay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}
```

### 2. Context Management

```javascript
class ConversationManager {
  constructor(maxContextLength = 4096) {
    this.messages = [];
    this.maxContextLength = maxContextLength;
  }

  addMessage(role, content) {
    this.messages.push({ role, content });
    this.trimContext();
  }

  trimContext() {
    // Estimate token count (rough approximation)
    let totalTokens = 0;
    let trimIndex = 0;

    for (let i = this.messages.length - 1; i >= 0; i--) {
      const tokens = this.estimateTokens(this.messages[i].content);
      totalTokens += tokens;
      
      if (totalTokens > this.maxContextLength) {
        trimIndex = i + 1;
        break;
      }
    }

    if (trimIndex > 0) {
      // Keep system message if present
      const systemMsg = this.messages.find(m => m.role === 'system');
      this.messages = systemMsg 
        ? [systemMsg, ...this.messages.slice(trimIndex)]
        : this.messages.slice(trimIndex);
    }
  }

  estimateTokens(text) {
    // Rough estimation: 1 token ≈ 4 characters
    return Math.ceil(text.length / 4);
  }

  getMessages() {
    return this.messages;
  }
}
```

### 3. Streaming Response Handler

```javascript
class StreamHandler {
  constructor(onChunk, onComplete, onError) {
    this.onChunk = onChunk;
    this.onComplete = onComplete;
    this.onError = onError;
    this.buffer = '';
  }

  async processStream(response) {
    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        this.buffer += decoder.decode(value, { stream: true });
        const lines = this.buffer.split('\n');
        
        // Keep last incomplete line in buffer
        this.buffer = lines.pop() || '';

        for (const line of lines) {
          if (line.trim()) {
            try {
              const data = JSON.parse(line);
              if (this.onChunk) {
                this.onChunk(data);
              }
            } catch (e) {
              console.error('JSON parse error:', e);
            }
          }
        }
      }

      // Process any remaining buffer
      if (this.buffer.trim()) {
        try {
          const data = JSON.parse(this.buffer);
          if (this.onChunk) {
            this.onChunk(data);
          }
        } catch (e) {
          console.error('Final buffer parse error:', e);
        }
      }

      if (this.onComplete) {
        this.onComplete();
      }

    } catch (error) {
      if (this.onError) {
        this.onError(error);
      }
    }
  }
}
```

---

## Production Deployment

### 1. Docker Deployment

```dockerfile
# Dockerfile
FROM ollama/ollama:latest

# Install models at build time
RUN ollama pull llama3.2
RUN ollama pull codellama

# Copy configuration
COPY config.yaml /etc/ollama/config.yaml

# Expose API port
EXPOSE 11434

# Start with authentication
CMD ["serve", "--config", "/etc/ollama/config.yaml"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  ollama:
    build: .
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_MODELS=/models
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  nginx:
    image: nginx:alpine
    ports:
      - "443:443"
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - ollama

volumes:
  ollama-data:
```

### 2. Nginx Configuration

```nginx
# nginx.conf
events {
  worker_connections 1024;
}

http {
  # Rate limiting
  limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
  limit_req_status 429;

  # Upstream
  upstream ollama {
    server ollama:11434;
    keepalive 32;
  }

  server {
    listen 80;
    server_name api.example.com;
    return 301 https://$server_name$request_uri;
  }

  server {
    listen 443 ssl http2;
    server_name api.example.com;

    # SSL Configuration
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;

    # Security Headers
    add_header X-Frame-Options "DENY" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Strict-Transport-Security "max-age=31536000" always;

    # CORS Configuration
    add_header Access-Control-Allow-Origin $http_origin always;
    add_header Access-Control-Allow-Methods "GET, POST, OPTIONS" always;
    add_header Access-Control-Allow-Headers "Authorization, Content-Type" always;
    add_header Access-Control-Max-Age 86400 always;

    # Handle preflight
    if ($request_method = OPTIONS) {
      return 204;
    }

    # API endpoints
    location /api/ {
      # Rate limiting
      limit_req zone=api burst=20 nodelay;

      # Authentication check (example with auth_request)
      auth_request /auth;
      auth_request_set $user $upstream_http_x_user;
      proxy_set_header X-User $user;

      # Proxy settings
      proxy_pass http://ollama;
      proxy_http_version 1.1;
      proxy_set_header Upgrade $http_upgrade;
      proxy_set_header Connection "upgrade";
      proxy_set_header Host $host;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto $scheme;

      # Timeouts
      proxy_connect_timeout 60s;
      proxy_send_timeout 60s;
      proxy_read_timeout 300s;

      # Buffering
      proxy_buffering off;
      proxy_request_buffering off;
    }

    # Auth endpoint (internal)
    location = /auth {
      internal;
      proxy_pass http://auth-service:3000/validate;
      proxy_pass_request_body off;
      proxy_set_header Content-Length "";
      proxy_set_header X-Original-URI $request_uri;
      proxy_set_header Authorization $http_authorization;
    }
  }
}
```

### 3. Kubernetes Deployment

```yaml
# ollama-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
      - name: ollama
        image: ollama/ollama:latest
        ports:
        - containerPort: 11434
        resources:
          requests:
            memory: "8Gi"
            cpu: "4"
            nvidia.com/gpu: 1
          limits:
            memory: "16Gi"
            cpu: "8"
            nvidia.com/gpu: 1
        volumeMounts:
        - name: models
          mountPath: /root/.ollama
        env:
        - name: OLLAMA_HOST
          value: "0.0.0.0"
        livenessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/tags
            port: 11434
          initialDelaySeconds: 10
          periodSeconds: 5
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: ollama-models-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: ollama-service
spec:
  selector:
    app: ollama
  ports:
  - port: 11434
    targetPort: 11434
  type: ClusterIP

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ollama-ingress
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - ollama.example.com
    secretName: ollama-tls
  rules:
  - host: ollama.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ollama-service
            port:
              number: 11434
```

---

## Security Checklist

### Essential Security Measures
- [ ] Never expose Ollama directly to the internet
- [ ] Implement authentication (Bearer token minimum)
- [ ] Use HTTPS with valid SSL certificates
- [ ] Enable rate limiting
- [ ] Implement IP whitelisting
- [ ] Add request/response logging
- [ ] Monitor for suspicious activity
- [ ] Regular security updates
- [ ] Implement CORS properly
- [ ] Use security headers

### OAuth 2.1 Compliance
- [ ] Implement PKCE for public clients
- [ ] Use secure token storage
- [ ] Implement token refresh
- [ ] Validate token signatures
- [ ] Check token expiration
- [ ] Implement proper scopes
- [ ] Use HTTPS for all OAuth flows
- [ ] Implement CSRF protection
- [ ] Log authentication events
- [ ] Handle token revocation

### API Security
- [ ] Input validation
- [ ] Output sanitization
- [ ] SQL injection prevention (if using DB)
- [ ] Command injection prevention
- [ ] Path traversal prevention
- [ ] XXE attack prevention
- [ ] Implement request size limits
- [ ] Timeout configuration
- [ ] Error message sanitization
- [ ] Audit logging

---

## Troubleshooting Guide

### Common Issues and Solutions

#### 1. Connection Refused
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Solutions:
- Start Ollama: ollama serve
- Check firewall settings
- Verify port binding
```

#### 2. CORS Errors
```javascript
// Add proper CORS headers in proxy
app.use((req, res, next) => {
  res.header('Access-Control-Allow-Origin', process.env.ALLOWED_ORIGIN);
  res.header('Access-Control-Allow-Headers', 'Content-Type, Authorization');
  res.header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
  next();
});
```

#### 3. Timeout Issues
```javascript
// Increase timeout for long-running requests
const response = await fetch(url, {
  signal: AbortSignal.timeout(300000) // 5 minutes
});
```

#### 4. Memory Issues
```bash
# Monitor memory usage
docker stats ollama

# Adjust Docker memory limits
docker run -m 16g ollama/ollama
```

#### 5. Model Loading Failures
```bash
# Check available disk space
df -h

# Clear model cache
rm -rf ~/.ollama/models

# Re-pull model
ollama pull llama3.2
```

---

## Performance Optimization

### 1. Model Configuration
```json
{
  "num_gpu": 1,
  "gpu_layers": 35,
  "num_thread": 8,
  "num_batch": 512,
  "context_length": 4096,
  "repeat_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 40,
  "top_p": 0.9
}
```

### 2. Caching Strategy
```javascript
class ResponseCache {
  constructor(ttl = 3600000) { // 1 hour
    this.cache = new Map();
    this.ttl = ttl;
  }

  generateKey(model, prompt, params) {
    return crypto
      .createHash('sha256')
      .update(`${model}:${prompt}:${JSON.stringify(params)}`)
      .digest('hex');
  }

  get(model, prompt, params) {
    const key = this.generateKey(model, prompt, params);
    const entry = this.cache.get(key);
    
    if (entry && Date.now() - entry.timestamp < this.ttl) {
      return entry.response;
    }
    
    return null;
  }

  set(model, prompt, params, response) {
    const key = this.generateKey(model, prompt, params);
    this.cache.set(key, {
      response,
      timestamp: Date.now()
    });
  }
}
```

### 3. Connection Pooling
```javascript
const http = require('http');
const https = require('https');

const agent = new http.Agent({
  keepAlive: true,
  keepAliveMsecs: 1000,
  maxSockets: 10,
  maxFreeSockets: 5
});

fetch(url, {
  agent: agent,
  // other options
});
```

---

## Resources and References

### Official Documentation
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Ollama API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Ollama JavaScript SDK](https://github.com/ollama/ollama-js)
- [Ollama Python SDK](https://github.com/ollama/ollama-python)

### OAuth 2.1 Resources
- [OAuth 2.1 Specification](https://datatracker.ietf.org/doc/html/draft-ietf-oauth-v2-1-07)
- [OAuth Security Best Practices](https://datatracker.ietf.org/doc/html/draft-ietf-oauth-security-topics)
- [PKCE Implementation](https://oauth.net/2/pkce/)

### Security Resources
- [OWASP API Security Top 10](https://owasp.org/www-project-api-security/)
- [NIST Cybersecurity Framework](https://www.nist.gov/cyberframework)
- [CVE Database](https://cve.mitre.org/)

### Community Resources
- [Ollama Discord](https://discord.gg/ollama)
- [Ollama Reddit](https://reddit.com/r/ollama)
- [Stack Overflow - Ollama Tag](https://stackoverflow.com/questions/tagged/ollama)

---

## Version History
- **v1.0** (August 2025): Comprehensive guide including OAuth 2.1, security, and deployment
- Based on Ollama v0.1.25+ specifications
- Incorporates 2025 security updates and best practices

---

*This guide provides comprehensive integration patterns for Ollama API with modern security practices including OAuth 2.1. Always verify against the latest official documentation and security advisories.*